Model Serving System (Week 9 - Day 2)

A production-ready minimal example of a machine learning serving system featuring a
FastAPI REST endpoint, batch inference script, Prometheus monitoring, and Docker packaging.

Project Structure

w9-day2/
├── app/
│ ├── **init**.py
│ ├── main.py # FastAPI application & logic
│ └── metrics.py # Prometheus instrumentation
├── data/
│ ├── input.csv # Sample input data
│ └── predictions.csv # Generated by batch_infer.py
├── models/
│ └── baseline.joblib # Trained model artifact
├── batch_infer.py # CLI script for batch processing
├── training.py # Helper script to generate the model
├── requirements.txt # Python dependencies
├── Dockerfile # Container definition
└── README.md # Documentation

Setup Instructions

1. Install Dependencies

It is recommended to use a virtual environment.

# Create and activate venv

python -m venv venv
source venv/bin/activate # On Windows: venv\Scripts\activate

# Install requirements

pip install -r requirements.txt

2. generate Model Artifact

Before running the app or batch inference, you need to generate the baseline.joblib file.

python training.py

This will create models/baseline.joblib.

Part 1: REST API

Run Locally

Start the Uvicorn server:

uvicorn app.main:app --reload

The server will start at http://127.0.0.1:8000.

API Usage Examples

1. Health Check

curl -X GET [http://127.0.0.1:8000/health](http://127.0.0.1:8000/health)

# Response: {"status": "ok"}

2. Make a Prediction

curl -X POST [http://127.0.0.1:8000/predict](http://127.0.0.1:8000/predict) \
 -H "Content-Type: application/json" \
 -d '{"x1": 1.5, "x2": 2.3}'

# Expected Response:

# {"score": 0.8521, "model_version": "v1.0"}

3. View Metrics

curl -X GET [http://127.0.0.1:8000/metrics](http://127.0.0.1:8000/metrics)

# Response: Prometheus formatted metrics (http_request_total, etc.)

Part 2: Batch Inference

Run the batch inference script using the sample data provided in data/input.csv.

python batch_infer.py data/input.csv data/predictions.csv

Output:
The script will generate data/predictions.csv containing the original columns plus prediction and model_version.

Part 3: Docker Instructions

1. Build the Image

Ensure you have run python train_model.py first so the models/ directory contains the model artifact.

docker build -t model-server:v1 .

2. Run the Container

docker run -p 8000:8000 model-server:v1

3. Test the Containerized App

You can use the same curl commands as the local setup.

# Test health

curl http://localhost:8000/health

# Test prediction

curl -X POST http://localhost:8000/predict \
 -H "Content-Type: application/json" \
 -d '{"x1": 0.5, "x2": 0.1}'
